\documentclass[11pt,a4paper]{article}

\usepackage{amsmath}  
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts} 
\usepackage{graphicx} 
\usepackage[usenames]{color}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{float}


\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclareMathOperator{\esssupp}{ess\,supp}


 \textwidth=16cm \hoffset = -1.9cm
 \lineskip=1.5\lineskip


% MATH -----------------------------------------------------------
\newcommand{\Real}{\mathbb R}
\newcommand{\E}{\mathbb{E}}
\newcommand{\s}{\mathbb{S}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\eps}{\varepsilon}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\nbr}{\mathrm{nbr}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\csimplex}{\bar{\mathcal{S}}^{d-1}}
\newcommand{\osimplex}{\mathcal{S}^{d-1}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\Hil}{\mathscr{H}}
\newcommand{\G}{\mathscr{G}}
\newcommand{\p}{\mathscr{P}}
\newcommand{\C}{\mathscr{C}}
\newcommand{\one}[1]{\mathbf{1}_{\{#1\}}}
\newcommand{\oneset}[1]{\mathbf{1}_{#1}}
\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\ind}{\mathrm{I}}
\newcommand{\D}{\mathscr{D}}
\newcommand{\Borel}{\mathscr{B}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\ds}{\displaystyle}
\newcommand{\voila}{\hfill $\blacksquare$}
\newcommand{\Id}{\mathrm{Id}}
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}


\begin{document}


\section{Sequential Importance Sampling with corrections for the AR1 model}

Let us first introduce our importance sampling idea using a simple example.

Consider a linear, normal and stationary AR(1) process for which

\[
x_{t} = \varphi x_{t-1} + \eps_{t}
\]
with $\eps_{t} \stackrel{iid}{\sim} \mathcal{N}(0, \sigma^{2})$.
Then

\[
x_{t} | x_{t-1} \sim \mathcal{N} (\varphi x_{t-1}, \sigma^{2}).
\]
This process is stationary provided that the initial distribution is

\[
x_{1} \sim \mathcal{N} \Bigg (0, \frac{\sigma^2}{1- \varphi^2} \Bigg )
\]
which is valid only if $|\varphi| < 1$.

Suppose that at any time $t$, only some subset of the values $\vec {x}^{(t)} = (x_1, \dots, x_{t})$ have so far been observed. However, those that have been observed are known without error. We collect the values that have been observed at or before time $t$ to form a vector $\vec{z}^{(t)} = (z_1^{(t)}, \dots, z_{t}^{(t)})$, where for each $s \leq t$, $z_s^{(t)} = x_s$ if $x_s$ has been observed at or before time $t$ and $z_s^{(t)} = -$ otherwise. Thus the coordinates of $\vec{z}^{(t)}$ belong to an extension of the reals $\mathbb{R} \cup \{ - \}$.

As time progresses, new observations become available. Thus some dashes in the vector $\vec{z}^{(t)}$ will be replaced by observations in the subsequent vector $\vec{z}^{(t+1)}$. However, the reverse does not occur: if we have $z_s^{(t)} = x_s$ for times $s$ and $t$ where $s \leq t$ then we must also have $z_s^{(t^{*})} = x_s$ at all subsequent times $t^{*} > t$.

We will therefore define a lower triangular $t \times t$ matrix $\vec{Z}^{(t)}$ in which row $i$ is the vector $\vec{z}^{(i)}$.

We will also define binary vectors $\vec{b}^{(t)} = (b_1^{(t)}, \dots, b_{t}^{(t)})$, where $b_s^{(t)} = 1$ if $z_s^{(t)} = x_s$, and $b_s^{(t)} = 0$ if $z_s^{(t)} = -$, and a lower triangular $t \times t$ binary matrix $\vec{B}^{(t)}$ in which row $i$ is the vector $\vec{b}^{(i)}$.
The matrix $\vec{B}^{(t)}$ must satisfy the constraints that if $B_{ij}^{(t)} = b_j^{(i)} = 1$ for $i \in \{ 1, \ldots, t \}$ and $j \in \{ 1, \ldots, i \}$, then $B_{mj}^{(t)} = 1$ for all $m \in \{ i+1, \ldots, t \}$.

Note $\vec{b}^{(t)}$ is fully determined by $\vec{z}^{(t)}$.

We will model $b_j^{(i)}$, when $b_j^{(i-1)} = 0$ as Bernoulli with probability $\theta$:
\[
b_j^{(i)} | b_j^{(i-1)} = 0 \sim Bernoulli(\theta).
\]


\subsection{Bayesian inference in a partially observed state space}

To perform Bayesian inference in this partially observed state space we want to determine the posterior distribution for the pairs $(\vec{x}^{(t)}, \vec{B}^{(t)})$ in light of observations $\vec{Z}^{(t)}$. Suppose that at time $t$ we have some prior density $q(\vec{x}^{(t)},\vec{B}^{(t)} | \vec{Z}^{(t-1)})$ that represents our knowledge about the state before we observe $\vec{z}^{(t)}$. This prior is defined on the subspace $\Omega = \Omega_1 \times \Omega_2$ where $\Omega_1 = \prod_{s=1}^t \mathbb{X}_s^{(t-1)} \subseteq \mathbb{R}^t$ and $\Omega_2 = \prod_{s=1}^t \mathbb{B}_s^{(t-1) \times (t-1)} \subseteq 2^{(t \times t)}$ is such that $\mathbb{X}_s^{(t-1)} = \mathbb{R}$ for $b_s^{(t-1)} = 0$ or $s=t$, and $\mathbb{X}_s^{(t-1)} = \{ x_s \}$ for $b_s^{(t-1)} = 1$. Also, $\vec{b}^{(t)}$ evolves independently of $\vec{x}^{(t)}$, so that the distribution of $\vec{b}^{(t)}$ is a conditional probability of the form $P(\vec{b}^{(t)} | \vec{x}^{(t)}, \vec{z}^{(t-1)} ) = P( \vec{b}^{(t)} | \vec{b}^{(t-1)} )$. Under these conditions, the posterior distribution for the pair $(\vec{x}^{(t)}, \vec{B}^{(t)})$ is obtained by re-normalising $q$ over values consistent with $\vec{Z}^{(t)}$. In other words, it has a density on the subspace $\Omega' = \Omega'_1 \times \Omega'_2$ where $\Omega'_1 = \prod_{s=1}^t \mathbb{X}_s^{(t)} \subseteq \mathbb{R}^t$ and and $\Omega'_2 = \prod_{s=1}^t \mathbb{B}_s^{(t \times t)} \subseteq 2^{(t \times t)}$ such that $\mathbb{X}_s^{(t)} = \mathbb{R}$ for $b_s^{(t)} = 0$ and $\mathbb{X}_s^{(t)} = \{ z_s^{(t)}\}$ for $b_s^{(t)} = 1$, given by:
\[
p(\vec{x}^{(t)}, \vec{B}^{(t)} | \vec{Z}^{(t)})  =   
\frac{ q(\vec{x}^{(t)}, \vec{B}^{(t)}|\vec{Z}^{(t-1)})} {r(\vec{z}^{(t)} | \vec{Z}^{(t-1)})}
\]
where 
\[
r(\vec{z}^{(t)} | \vec{Z}^{(t-1)}) = \int q(\vec{x}^{(t)}, \vec{B}^{(t)}|\vec{Z}^{(t-1)}) \prod_{\{s \leq t: b_s^{(t)} = 0\}} \diff x_s^{(t)}.
\]

This follows from Bayes' Rule, but we omit the proof to keep the example simple. A proof is provided below.



The prior for time $t$, $q(\vec{x}^{(t)},\vec{B}^{(t)} | \vec{Z}^{(t-1)})$ represents our knowledge about the state before we observe $\vec{z}^{(t)}$. In our toy example, using the independence of the processes $x$ and $B$, we have
\begin{equation}
q(\vec{x}^{(t)},\vec{B}^{(t)} | \vec{Z}^{(t-1)}) = q(\vec{x}^{(t-1)},\vec{B}^{(t-1)} | \vec{Z}^{(t-1)}) p(x_t | x_{t-1}) p(\vec{b}^{(t)} | \vec{b}^{(t-1)}).
\end{equation}





\subsection{Sequential importance sampling with an auxiliary variable in a partially observed state space}

We want to approximate the distribution $p(\vec{x}^{(t)}, \vec{B}^{(t)} | \vec{Z}^{(t)})$ iteratively, as well as related properties including $p(x_{t}, \vec{b}^{(t)} | \vec{Z}^{(t)})$ and expectations of the form:
\[
E_{p} \Big[f(\vec{x}^{(t)}, \vec{B}^{(t)}) \Big] = \int f(\vec{x}^{(t)}, \vec{B}^{(t)})\; p(\vec{x}^{(t)}, \vec{B}^{(t)} | \vec{Z}^{(t)})\; \diff \vec{x}^{(t)}
\]
for functions $f: \Omega  \rightarrow \Real$. 

We take a sequential importance sampling approach, at each iteration using a collection of weighted particles to represent $p(\vec{x}^{(t-1)}, \vec{B}^{(t-1)} | \vec{Z}^{(t-1)})$ from the previous iteration, in the sense that these particles can be used to construct weighted Monte Carlo estimates for integrals of the above form. We evolve these under the model to create a new set of weighted particles representing the prior for iteration $t$, namely $q(\vec{x}^{(t)}, \vec{B}^{(t)} | \vec{Z}^{(t-1)})$. We then modify these particles to be consistent with the observations $\vec{Z}^{(t)}$, and adjust the weights to ensure the resulting weighted particles represent $p(\vec{x}^{(t)}, \vec{B}^{(t)} | \vec{Z}^{(t)})$.

Consider a particle $(\vec{x}^{(t-1)},\vec{B}^{(t-1)})$ constructed at time point $t$, one of $n$ such particles. We generate a new value $x_t$ for this particle by drawing $p(x_t | x_{t-1})$. Similarly, we generate a new value $\vec{b}^{(t)}$ for this particle by drawing from $p(\vec{b}^{(t)} | \vec{b}^{(t-1)})$ (see equation $(1)$). However, the new vector of observations $\vec{z}^{(t)}$ is typically inconsistent with a particle $(\vec{x}^{(t)}, \vec{B}^{(t)})$ thus constructed, in two ways: first, the coordinates at which $\vec{b}^{(t)}$ contains a 0 may not correspond to the coordinates at which $\vec{z}^{(t)}$ contains a `-', and second, the observed values in $\vec{z}^{(t)}$ differ from the corresponding coordinates of $\vec{x}^{(t)}$. We must therefore correct $\vec{x}^{(t)}$ and $\vec{b}^{(t)}$ in light of the new observation $\vec{z}^{(t)}$. The simplest way to do this is to first replace $\vec{b}^{(t)}$ with the unique binary vector $\vec{b'}^{(t)}$ that is consistent with $\vec{z}^{(t)}$, and then replace the coordinates of $\vec{x}^{(t)}$ with the corresponding coordinates of $\vec{z}^{(t)}$ wherever $\vec{b'}^{(t)}$ has a `1', thus generating a corrected term $\vec{x'}^{(t)}$. The corrections thus made at time point $t$ will be carried forward into the particles used at all future times. Note that in this simple example the corrections are deterministic, that is, $\vec{x'}^{(t)}$ and $\vec{b'}^{(t)}$ are deterministic functions of $\vec{x}^{(t)}$ and $\vec{b}^{(t)}$. 

At each iteration, in order to simulate via a two-step process in which we generate a sample and then correct in light of data, we propose to introduce an auxiliary variable into importance sampling. The auxiliary variable will be the yet to be corrected sample $(\vec{x}^{(t)}, \vec{B}^{(t)})$ at time $t$. We will use a projection map $\pi$ to relate the augmented space $\Omega_{\vec{Z}^{(t)}}$ {\color{red} (HHH Clarify the definition of $\Omega_{\vec{Z}^{(t)}}$ and explain why it has a subscript $\vec{Z}^{(t)}$)} containing the elements $(\vec{x'}^{(t)}, \vec{B'}^{(t)}, \vec{x}^{(t)}, \vec{B}^{(t)})$ to the corrected state space $\Omega'$ containing $(\vec{x'}^{(t)}, \vec{B'}^{(t)})$. Then, we apply importance sampling to be able to estimate the posterior distribution $p(\vec{x}^{(t)}, \vec{B}^{(t)} | \vec{Z}^{(t)})$ or the expectation $E_{p}[f(\vec{x}^{(t)},\vec{B}^{(t)})]$  sampling from the known distribution $q$.

For simplicity, let us call $(\vec{x}^{(t)}, \vec{B}^{(t)}) = \vec{y}^{(t)}$ and $(\vec{x'}^{(t)}, \vec{B'}^{(t)}) = \vec{y'}^{(t)}$

Our strategy is to define a probability $p^*$ on $\Omega_{\vec{z}^{(t)}}$ such that the marginal distribution of $p^*$ on $\Omega'$ is $p$, that is:

\[
p(\vec{y'}^{(t)} | \vec{Z}^{(t)}) = \int p^*(\vec{y'}^{(t)}, \vec{y}^{(t)} | \vec{Z}^{(t)})\; \diff \vec{y}^{(t)} \vec{y}^{(t)}
\]
By the the disintegration theorem there exist a family of measures $d\nu_{y'}(y)$ on $\Omega_{\vec{z}^{(t)}}$ such that for every measurable Borel function $g : \Omega_{\vec{z}^{(t)}} \rightarrow [0,\infty]$:

\[
\int_{\Omega_{\vec{z}^{(t)}}} g(\vec{y}', \vec{y})\diff (\vec{y'},\vec{y}) = \int_{\Omega'}\int_{\pi^{-1}(\vec{y'})} g(\vec{y}', \vec{y}) \diff \vec{y} \diff \vec{\vec{y'}}
\]
therefore

\begin{align*}
E_{p}[f(\vec{y'}^{(t)})]  & = \int_{\Omega'} f(\vec{y'}^{(t)})\; p(\vec{y'}^{(t)} | \vec{Z}^{(t)}) \diff \vec{y'}^{(t)} = \int_{\Omega'} f(\vec{y'}^{(t)})\Bigg[\int_{\pi^{-1}(\vec{y'})} p^*(\vec{y'}^{(t)}, \vec{y}^{(t)} | \vec{Z}^{(t)}) \diff \vec{y}^{(t)}\Bigg] \diff \vec{y'}^{(t)} \\ 
& = \int_{\Omega_{\vec{Z}^{(t)}}} f(\pi(\vec{y'}^{(t)}, \vec{y}^{(t)}))\; p^*(\vec{y'}^{(t)}, \vec{y}^{(t)} | \vec{Z}^{(t)}) \diff (\vec{y'}^{(t)},\vec{y}^{(t)}) = E_{p^*}[f(\pi (\vec{y'}^{(t)}, \vec{y}^{(t)}))].
\end{align*}
Using importance sampling

\begin{align*}
E_{p^*}[f(\pi (\vec{y'}^{(t)}, \vec{y}^{(t)})] & = \int_{\Omega_{\vec{Z}^{(t)}}} f(\pi(\vec{y'}^{(t)},  \vec{y}^{(t)})) \; \frac{p^*(\vec{y'}^{(t)}, \vec{y}^{(t)}|\vec{Z}^{(t)})}{q(\vec{y'}^{(t)}, \vec{y}^{(t)} | \vec{Z}^{(t-1)})} q(\vec{y'}^{(t)}, \vec{y}^{(t)} | \vec{Z}^{(t-1)})\; \diff \vec{y}^{(t)} \\ &\approx \sum_{j=1}^n  w^{(t)_j}f(\pi (\vec{y'}^{(t)_j}, \vec{y}^{(t)_j}))
\end{align*}
{\color{blue} (HHH Should the particle index $j$ be a subscript on $\vec{y}$ instead of $(t)$?)}
where the weights $w^{(t)_j}$ are:

\[
w^{(t)_j} = \frac{p^*(\vec{y'}^{(t)_j}, \vec{y}^{(t)_j} | \vec{Z}^{(t)})} {q(\vec{y'}^{(t)_j}, \vec{y}^{(t)_j}|\vec{Z}^{(t-1)})}
\]
In our example with deterministic substitutions $p^*$ will simply be:

\[
p^*(\vec{y'}^{(t)}, \vec{y}^{(t)} | \vec{Z}^{(t)}) = p(\vec{y'}^{(t)} | \vec{Z}^{(t)}) = \frac{ q(\vec{y'}^{(t)}|\vec{Z}^{(t-1)})} {r(\vec{Z}^{(t)} | \vec{Z}^{(t-1)})}
\]
and

\[
q(\vec{y'}^{(t)}, \vec{y}^{(t)} | \vec{Z}^{(t-1)}) = q(\vec{y}^{(t)} | \vec{Z}^{(t-1)})
\]
therefore the normalised weights $w^{(t)_j}$ will be:

\begin{align*}
W^{(t)_j} & = \frac{q(\vec{y'}^{(t)_j} | \vec{Z}^{(t-1)}) }{q(\vec{y}^{(t)_j} | \vec{Z}^{(t-1)})}\Bigg( \sum_{j=1}^n  \frac{q(\vec{y'}^{(t)_j} | \vec{Z}^{(t-1)}) }{q(\vec{y}^{(t)_j} | \vec{Z}^{(t-1)})}\Bigg)^{-1} \\
& = \frac{q(\vec{x'}^{(t)_j}, \vec{B'}^{(t)_j} | \vec{Z}^{(t-1)}) }{q(\vec{x}^{(t)_j}, \vec{B}^{(t)_j} | \vec{Z}^{(t-1)})}\Bigg( \sum_{j=1}^n \frac{q(\vec{x'}^{(t)_j}, \vec{B'}^{(t)_j} | \vec{Z}^{(t-1)}) }{q(\vec{x}^{(t)_j}, \vec{B}^{(t)_j} | \vec{Z}^{(t-1)})}\Bigg)^{-1}
\end{align*}
since the constant $r$ of $q(\vec{x}^{(t-1)_j},\vec{B}^{(t-1)_j} | \vec{Z}^{(t-1)})$ cancels out after normalisation.

Notice that since we are correcting every previous event at every iteration, the weights for the previous times too will need to be recalculated at every iteration and that we cannot multiply the weights for the last iteration by the weights calculated at the previous time as we normally do in Sequential Importance Sampling. However, we will now show that there will be many cancellations in the calculation of the weights, making the process manageable and not computationally expensive.

For every particle $j$, the unnormalised weights at time $t$ for $b' \in \{0,1\}$ and $b \in \{0,1\}$ are calculated as follows:

\[
w^{(t)_j} = \frac{q(\vec{y'}^{(t)} | \vec{Z}^{(t-1)}) }{q(\vec{y}^{(t)} | \vec{Z}^{(t-1)})} = \frac{\bigg \{ \prod_{i=2}^{t}  \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \bigg [ { - \frac{1}{2 \sigma^{2}} }  (x'_{i} - \varphi x'_{i-1})^{2} \bigg ] \bigg \} p(x'_{1}) \prod_{i=1}^{t} \theta^{b'_i} (1 - \theta)^{1-b'_{i}}  }{\bigg \{ \prod_{i=2}^{t}  \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \bigg [ { - \frac{1}{2 \sigma^{2}} }  (x_{i} - \varphi x_{i-1})^{2} \bigg ] \bigg \} p(x_{1}) \prod_{i=1}^{t} \theta^{b_i} (1 - \theta)^{1-b_{i}} }
\]
where the normalisation constants will cancel out.
Notice that we will have terms cancellations in the Gaussian term when both $z_{i-1} \neq 0$ and $z_{i} \neq 0$, while the Bernoulli term will cancel every time that $z_{i} \neq 0$. Also notice that $p(x'_1) = p(x_1)$, and those two terms will cancel out as well.



\subsection{The pseudo-code for the AR(1) model with deterministic corrections}

\begin{algorithm}[H]
\caption{SIR with deterministic corrections for an AR(1) model}\label{euclid}
 \begin{algorithmic}

 \State  \bf{Initialize:} \normalfont At time i = 1
            
\begin{enumerate}
	\item For $j = 1, \dots , n$
	\begin{enumerate}
		\item Sample $x_{1}^{(j)} \sim \mathcal{N} \Bigg (0, \frac{\sigma^2}{1- \varphi^2} \Bigg)$ and sample $b_1^{(j)} \sim Bernoulli(\theta)$
		\item Evaluate the importance weights up to a normalising constant:
		\[
		\tilde{w}^{(j)}_{1} = 1
		\]
	\end{enumerate}
	\item For $j = 1, \dots , n$ normalise the importance weights: 
	\[
	w^{(j)}_{1} = \frac{1}{n}
	\]
\end{enumerate}

 \State  \bf{Iterate:} \normalfont For $i$ from 2 to $N$

\begin{enumerate}
	\item For $j = 1, \dots , n$
	\begin{enumerate}
  		\item sample $x_{i}^{(j)} \sim \mathcal{N} (\varphi x_{i-1}^{(j)}, \sigma^{2})$ and $\vec{b}_i^{(j)} \sim Bernoulli(\theta)$.
		\item {If we had new observations, correct every element of $\vec{x}_{i}^{(j)}$ in light of the data $\vec{z}_{i}$ to obtain new samples $\vec{x'}_{i}^{(j)}$ directly substituting the new data: for $m = 1, \dots ,i$ if $z_{m} \neq 0$, $(x')_m^{(j)} = z_{m}$ else if $z_{m} = 0$, $(x')_m^{(j)} = x_m^{(j)}$.}
		\item Evaluate the importance weights up to a normalising constant:
		\[
		\tilde{w}^{(j)}_{i} = \frac{q(\vec{x'}^{(i)_j}, \vec{B'}^{(i)_j} | \vec{Z}^{(i-1)})}{q(\vec{x}^{(i)_j}, \vec{B}^{(i)_j} | \vec{Z}^{(i-1)})}
		\]
	\end{enumerate}
	\item For $j = 1, \dots , n$ normalise the importance weights:
	\[
	w^{(j)}_{i} = \frac{\tilde{w}^{(j)}_{i}}{\sum_{k=1}^{n}\tilde{w}^{(k)}_{i}}
	\]
	\item Perform resampling
	\begin{enumerate}
	    \item Draw N particles from the current particle set with probabilities proportional to their weights. Replace the current particle set with this new one
	    \item For $j=1,\cdots ,n$ set $w_{i}^{(j)}=1/n$
	\end{enumerate}
	

\end{enumerate}

\State  \bf{Output:} \normalfont $\Big \{ x_{i}^{(j)} , w_{i}^{(j)} \Big \}_{j = 1}^{n}$ for $i \in \{ 1, N \}$

\item Calculate the expectation  $E[x_i] = \sum_{k=1}^{n} x^{(k)}_i w_{i}^{(k)}$
  
 \end{algorithmic}
\end{algorithm}

\section{The non-deterministic corrections}

In this section we will add non-deterministic corrections to our algorithm. Specifically, every substitution we make based on observations will have an effect on the simulated adjacent unobserved events: the new algorithm will substitute each observed elements and also correct the previous element and the following element if they are unobserved. We will be sampling an unobserved event at times $s\leq t$ where $t$ is the current time, with probability $H(x_s|x'_{s-1}, x'_{s+1})$.

The probability $H(x_s|x'_{s-1}, x'_{s+1})$ will have the form:

\[
H(x_s|x'_{s-1}, x'_{s+1}) = \begin{cases} p(x_s|x'_{s-1}), & \mbox{if} \quad b_s = 0  \quad \mbox{and} \quad b'_{s-1} = 1 \quad \mbox{and} \quad b'_{s+1} = 0\\ 
p(x_s|x'_{s+1}), & \mbox{if} \quad b_s = 0  \quad \mbox{and} \quad b_{s-1} = 0 \quad \mbox{and} \quad b'_{s+1} = 1\\
\frac{p(x_s|x'_{s-1})p(x'_{s+1}|x_s)}{p(x_s)}, & \mbox{if} \quad b'_s = 0  \quad \mbox{and} \quad b'_{s-1} = 1 \quad \mbox{and} \quad b'_{s+1} = 1\\
1 & \mbox{otherwise} \end{cases}
\]
We already know that $p(x_s|x_{s-1}) \sim \mathcal{N} (\varphi x_{s}, \sigma^{2})$. Also, for a stationary AR(1) model, the marginal distribution for every value of $s$ is $p(x_{s}) \sim \mathcal{N}(0, v)$ where $v = Var(x_s)$ is the variance of $x_s$. Since the process is stationary, (linear and Gaussian), it is also time reversible and using Bayes theorem it can be shown that $p(x_{s}|x_{s+1}) \sim \mathcal{N} (\varphi x_{s+1}, \sigma^{2})$ as well (VVV shell I expand on this?).
Then $\frac{p(x_s|x_{s-1})p(x_{s+1}|x_s)}{p(x_s)} \sim $

This new terms will be introduced in the calculation of the weights:

\[
w^{(t)} = \frac{q(\vec{x'}^{(t)_j}, \vec{B'}^{(t)_j} | \vec{Z}^{(t-1)}) }{q(\vec{x}^{(t)_j}, \vec{B}^{(t)_j} | \vec{Z}^{(t-1)}) H(\vec{x}^{(t)_j}|\vec{x'}^{(t)_j}) }
\]

\section{derivation of the posterior for a partially observed state}

\begin{proof}

by conditional probability we have

\[
p(\vec{x}^{(t)},\vec{B}^{(t)} | \vec{Z}^{(t)}) = \frac{p(\vec{x}^{(t)},\vec{B}^{(t)}, \vec{Z}^{(t)})}{p(\vec{Z}^{(t)})} = \frac{p(\vec{x}^{(t)},\vec{B}^{(t)}, \vec{z}^{(t)}, \vec{Z}^{(t-1)})}{p(\vec{z}^{(t)},\vec{Z}^{(t-1)})}
\]
the numerator of this equation can be written as follows:

\begin{multline*}
p(\vec{x}^{(t)},\vec{B}^{(t)}, \vec{z}^{(t)}, \vec{Z}^{(t-1)}) = p(\vec{z}^{(t)}, \vec{Z}^{(t-1)} | \vec{x}^{(t)},\vec{B}^{(t)})p(\vec{x}^{(t)},\vec{B}^{(t)}) = \\ p(\vec{z}^{(t)} | \vec{x}^{(t)},\vec{B}^{(t)}) p(\vec{Z}^{(t-1)} | \vec{x}^{(t)},\vec{B}^{(t)})p(\vec{x}^{(t)},\vec{B}^{(t)}) = p(\vec{z}^{(t)} | \vec{x}^{(t)},\vec{B}^{(t)}) p(\vec{Z}^{(t-1)}, \vec{x}^{(t)} , \vec{B}^{(t)})
\end{multline*}
and the denominator will be

\[
p(\vec{z}^{(t)},\vec{Z}^{(t-1)}) = p(\vec{z}^{(t)} | \vec{Z}^{(t-1)}) p(\vec{Z}^{(t-1)}) 
\]
so the posterior can be written

\begin{multline*}
p(\vec{x}^{(t)},\vec{B}^{(t)} | \vec{Z}^{(t)}) = \frac{p(\vec{z}^{(t)} | \vec{x}^{(t)},\vec{B}^{(t)}) p(\vec{Z}^{(t-1)}, \vec{x}^{(t)} , \vec{B}^{(t)})}{p(\vec{z}^{(t)} | \vec{Z}^{(t-1)}) p(\vec{Z}^{(t-1)}) } = \frac{p(\vec{z}^{(t)} | \vec{x}^{(t)},\vec{B}^{(t)}) p(\vec{x}^{(t)} , \vec{B}^{(t)} | \vec{Z}^{(t-1)})}{p(\vec{z}^{(t)} | \vec{Z}^{(t-1)})}
\end{multline*}
Since the observations at time $t$ are made without error and $\vec{z}^{(t)}$ is completely determined by $\vec{x}^{(t)}$ and $\vec{B}^{(t)}$ we have

\[
p(\vec{z}^{(t)} | \vec{x}^{(t)}, \vec{B}^{(t)}) = 1
\]
The posterior then becomes

\[
p(\vec{x}^{(t)},\vec{B}^{(t)} | \vec{Z}^{(t)}) = \frac{q(\vec{x}^{(t)} , \vec{B}^{(t)} | \vec{Z}^{(t-1)})}{r(\vec{z}^{(t)} | \vec{Z}^{(t-1)})}
\]
with the marginal distribution being:

\[
r(\vec{z}^{(t)} | \vec{Z}^{(t-1)}) = \int_{\Omega'} q(\vec{x}^{(t)}, \vec{B}^{(t)}|\vec{Z}^{(t-1)}) \diff x_{1:t} = \int q(\vec{x}^{(t)}, \vec{B}^{(t)}|\vec{Z}^{(t-1)}) \prod_{\{s \leq t: b_s^{(t)} = 0\}} \diff x_s^{(t)}
\]
since at each time point $s$ where we have an observation we will have $b^{(t)}_s=1$ and the value of $x_s$ will be fixed and equal to $z^{(t)}_s$.

\end{proof}

\end{document}

